# =============================================================================
# Palestine Catwatch - Environment Variables
# =============================================================================
# Copy this file to .env and fill in your values
# NEVER commit .env to version control!
# =============================================================================

# ============================================
# Environment
# ============================================
# Set to 'production' or 'prod' in production to enable security validations
ENVIRONMENT=development

# ============================================
# CORS Configuration
# ============================================
# Comma-separated list of allowed origins for cross-origin requests
# In production, set this to your actual frontend domain(s)
# Example: ALLOWED_ORIGINS=https://example.com,https://www.example.com
# Note: In development mode, CORS allows all origins regardless of this setting
ALLOWED_ORIGINS=http://localhost:3000,http://localhost:5173

# =============================================================================
# DATABASE
# =============================================================================

# PostgreSQL connection string (required)
# Format: postgresql://user:password@host:port/database?sslmode=require
# For Neon: postgresql://user:password@ep-xxx.region.aws.neon.tech/neondb?sslmode=require
DATABASE_URL=postgresql://user:password@localhost:5432/protest_db

# =============================================================================
# AUTHENTICATION & SECURITY
# =============================================================================

# JWT Secret Key (REQUIRED for production)
# Generate with: python -c "import secrets; print(secrets.token_hex(32))"
# MUST be changed from default in production!
JWT_SECRET_KEY=dev-secret-key-change-in-production

# Refresh token secret (7-day tokens) - MUST be different from JWT_SECRET_KEY
JWT_REFRESH_SECRET_KEY=your-refresh-secret-key-here-min-32-chars

# JWT Token Expiration
JWT_EXPIRE_MINUTES=30
JWT_REFRESH_EXPIRE_DAYS=7

# ============================================
# Email Verification
# ============================================
# Set to 'false' to disable email verification requirement (for development)
REQUIRE_EMAIL_VERIFICATION=true

# ============================================
# Account Security
# ============================================
# Account lockout after failed login attempts
MAX_FAILED_LOGIN_ATTEMPTS=5
LOCKOUT_DURATION_MINUTES=15
FAILED_LOGIN_RESET_MINUTES=30

# CSRF Protection (disable for development if needed)
CSRF_ENABLED=true

# Cloudflare Turnstile (CAPTCHA)
# Get your keys at: https://dash.cloudflare.com/ -> Turnstile
# Site key goes in frontend (src/components/Turnstile.jsx)
# Secret key goes here for server-side verification
TURNSTILE_SECRET_KEY=your-turnstile-secret-key

# ============================================
# Database Migrations
# ============================================
# Required to run destructive migrations (e.g., dropping tables with data)
# Set to 'true' ONLY when you intentionally want to delete data
ALLOW_DESTRUCTIVE_MIGRATION=false

# =============================================================================
# ANTHROPIC API (Claude Vision)
# =============================================================================

# Anthropic API key for uniform recognition feature
# Required for Claude Vision API to analyze police uniforms, equipment, and insignia.
# Get your key at: https://console.anthropic.com/
# Cost: ~$0.015 per image analysis (Claude claude-sonnet-4-20250514 with vision)
ANTHROPIC_API_KEY=sk-ant-api03-...

# Auto-analyze uniforms during processing
# true: Every detected officer is automatically analyzed (higher API costs)
# false: Analysis is on-demand only via /appearances/{id}/analyze endpoint
# Default: false (to prevent unexpected API charges)
ENABLE_AUTO_UNIFORM_ANALYSIS=false

# Rate limit for uniform analysis API calls (requests per minute)
# Lower values (5-10): Safer for cost control, slower processing
# Higher values (20-30): Faster processing, higher potential costs
# Default: 10
UNIFORM_ANALYSIS_RATE_LIMIT=10

# =============================================================================
# RATE LIMITING & DOS PROTECTION
# =============================================================================

# Maximum concurrent AI processing tasks per IP address
# Prevents single users from consuming all processing resources
# Default: 3
MAX_CONCURRENT_AI_TASKS=3

# Maximum upload sizes (in MB)
# Default: 50MB for images, 500MB for videos
MAX_IMAGE_SIZE_MB=50
MAX_VIDEO_SIZE_MB=500

# =============================================================================
# SOCKET.IO ROOM MANAGEMENT
# =============================================================================

# Delay (seconds) before cleaning up completed task rooms
# Default: 300 (5 minutes)
SIO_ROOM_CLEANUP_DELAY=300

# Maximum age (hours) for any room before forced cleanup
# Prevents memory leaks from abandoned sessions
# Default: 24
SIO_ROOM_MAX_AGE_HOURS=24

# How often (seconds) to run the cleanup sweep
# Default: 3600 (1 hour)
SIO_CLEANUP_INTERVAL=3600

# Maximum number of concurrent tracked rooms
# Memory protection limit
# Default: 1000
SIO_MAX_ROOMS=1000

# =============================================================================
# FACE MATCHING
# =============================================================================

# Face matching mode: strict, moderate, or loose
# strict: 0.60 threshold - Very few false positives, may miss matches
# moderate: 0.70 threshold - Balanced (recommended)
# loose: 0.80 threshold - More matches, more false positives
# Default: moderate
FACE_MATCH_MODE=moderate

# =============================================================================
# AI DETECTION THRESHOLDS
# =============================================================================
# These thresholds control the minimum confidence required for various detections.
# Higher values = fewer false positives but may miss valid detections.
# Lower values = more detections but increased false positives.

# Minimum confidence for face detection (0.0 - 1.0)
# Default: 0.6 (60% confidence)
FACE_DETECTION_CONFIDENCE=0.6

# Minimum confidence for police force AI detection (0.0 - 1.0)
# Forces detected with confidence below this won't be auto-populated
# Default: 0.6 (60% confidence)
FORCE_DETECTION_CONFIDENCE=0.6

# Minimum confidence for police rank AI detection (0.0 - 1.0)
# Default: 0.6 (60% confidence)
RANK_DETECTION_CONFIDENCE=0.6

# Minimum confidence for YOLO person detection (0.0 - 1.0)
# Default: 0.4 (40% confidence - lower to catch partial views)
PERSON_DETECTION_CONFIDENCE=0.4

# =============================================================================
# PROCESSING LIMITS (DoS Protection)
# =============================================================================

# Maximum number of officers to process per image (DoS protection)
# Prevents resource exhaustion from images with many detections
# Default: 50 (reasonable for protest images)
MAX_OFFICERS_PER_IMAGE=50

# Maximum number of frames to process per video (DoS protection)
# Default: 1000
MAX_FRAMES_PER_VIDEO=1000

# =============================================================================
# YOUTUBE DOWNLOAD (cookies.txt)
# =============================================================================
# For best YouTube download reliability, export browser cookies to backend/cookies.txt
#
# WHY: YouTube blocks automated downloads. Browser cookies prove you're a real user
#      and bypass age restrictions, geo-blocks, and bot detection.
#
# HOW TO EXPORT COOKIES:
# 1. Install browser extension: "Get cookies.txt LOCALLY" (Chrome/Firefox)
# 2. Go to youtube.com and ensure you're logged in
# 3. Click the extension icon and "Export" cookies
# 4. Save as: backend/cookies.txt (Netscape format)
#
# SECURITY NOTE: cookies.txt contains session tokens - keep it private!
# The file is already in .gitignore to prevent accidental commits.
#
# WITHOUT COOKIES: Many videos will fail with "403 Forbidden" errors
# WITH COOKIES: Most public YouTube videos should download successfully

# =============================================================================
# LOGGING
# =============================================================================

# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
# Default: INFO
LOG_LEVEL=INFO

# Log format: json (structured) or text (human-readable)
# Use "json" for production (parseable by log aggregators)
# Use "text" for development (colored, readable)
# Default: json
LOG_FORMAT=text

# Optional log file path (in addition to stdout)
# LOG_FILE=/var/log/palestine-catwatch/app.log

# Log rotation settings (only apply when LOG_FILE is set)
# Maximum size of each log file before rotation
# Format: <size>M or <size>K (e.g., 10M = 10 megabytes)
# Default: 10M
LOG_MAX_SIZE=10M

# Number of backup log files to keep
# Older files are deleted when limit is exceeded
# Default: 5
LOG_BACKUP_COUNT=5

# =============================================================================
# FILE CLEANUP
# =============================================================================

# Cleanup job paths (override defaults if needed)
# CLEANUP_MEDIA_DIR=data/media
# CLEANUP_CROPS_DIR=data/crops
# CLEANUP_CACHE_DIR=data/cache
# CLEANUP_DOWNLOADS_DIR=downloads

# Cleanup thresholds
# How long (days) before orphaned files are deleted
CLEANUP_MAX_ORPHAN_AGE_DAYS=7

# How long (hours) before temp files are deleted
CLEANUP_MAX_TEMP_AGE_HOURS=24

# How long (days) before cache files expire
CLEANUP_MAX_CACHE_AGE_DAYS=30

# =============================================================================
# FRONTEND
# =============================================================================
# (Set these in frontend/.env or at build time)

# API URL for frontend to connect to backend
# VITE_API_URL=http://localhost:8000

# =============================================================================
# MONITORING & HEALTH CHECKS
# =============================================================================

# Health check endpoints are available at:
# - GET /health         - Basic health check
# - GET /health/db      - Database connectivity
# - GET /health/ready   - Full readiness check

# External monitoring service webhook (optional)
# Called on critical errors for alerting
# ALERT_WEBHOOK_URL=https://hooks.slack.com/services/xxx/yyy/zzz

# Metrics export interval (seconds) - for stats collection
# Default: 60
# METRICS_INTERVAL=60

# =============================================================================
# CLOUDFLARE R2 STORAGE (Recommended for Production)
# =============================================================================
# R2 provides persistent file storage that survives server restarts/deploys.
# Without R2, files are stored locally and will be lost on ephemeral hosts (Railway, Render, etc.)

# Cloudflare Account ID (found in Cloudflare dashboard URL)
R2_ACCOUNT_ID=your-account-id-here

# R2 API Token credentials (create at: R2 > Manage R2 API Tokens > Create API Token)
# Required permissions: Object Read & Write for your bucket
R2_ACCESS_KEY_ID=your-access-key-id
R2_SECRET_ACCESS_KEY=your-secret-access-key

# Bucket name (the name you gave your R2 bucket)
R2_BUCKET_NAME=your-bucket-name

# Public URL for the bucket (optional, enables direct R2 URLs for frontend)
# Options:
# 1. R2.dev subdomain: Enable "Public Access" in bucket settings, get URL like:
#    https://pub-xxxxxxxxxxxx.r2.dev
# 2. Custom domain: Connect your own domain in bucket settings
# If not set, files are served through your backend (/data/... routes)
R2_PUBLIC_URL=https://pub-xxxxxxxxxxxx.r2.dev

# =============================================================================
# BACKUP & RECOVERY
# =============================================================================

# Database backup settings (for pg_dump scripts)
# BACKUP_DIR=/var/backups/palestine-catwatch
# BACKUP_RETENTION_DAYS=30

# Media backup location (S3 or local)
# MEDIA_BACKUP_PATH=/mnt/backup/media
# S3_BACKUP_BUCKET=my-backup-bucket

# To create a database backup:
#   pg_dump $DATABASE_URL > backup_$(date +%Y%m%d).sql
#
# To restore from backup:
#   psql $DATABASE_URL < backup_20240101.sql

# =============================================================================
# DUPLICATE DETECTION
# =============================================================================

# Perceptual hash similarity threshold (Hamming distance)
# Lower = stricter matching (fewer false positives)
# Higher = looser matching (may catch more duplicates)
# Default: 10 (out of 256 bits)
DUPLICATE_SIMILARITY_THRESHOLD=10

# Maximum images to scan for perceptual duplicates
# Prevents excessive scanning on large databases
# Default: 100000
DUPLICATE_MAX_SCAN=100000

# =============================================================================
# PRODUCTION CHECKLIST
# =============================================================================
# Before deploying to production, ensure:
#
# SECURITY:
# [ ] JWT_SECRET_KEY is changed to a secure random value
# [ ] JWT_REFRESH_SECRET_KEY is changed to a secure random value
# [ ] CSRF_ENABLED=true
# [ ] ENVIRONMENT=production
# [ ] ALLOWED_ORIGINS is set to production domains only
#
# DATABASE:
# [ ] DATABASE_URL points to production database
# [ ] Database backups are configured and tested
# [ ] Alembic migrations have been run: alembic upgrade head
#
# API KEYS:
# [ ] ANTHROPIC_API_KEY is valid (if using uniform analysis)
#
# RATE LIMITING:
# [ ] Rate limits are appropriate for expected traffic
# [ ] MAX_CONCURRENT_AI_TASKS is set based on server capacity
# [ ] SIO_MAX_ROOMS is set based on expected concurrent users
#
# MONITORING:
# [ ] LOG_FILE is set to a persistent location
# [ ] Log rotation is configured (LOG_MAX_SIZE, LOG_BACKUP_COUNT)
# [ ] Health check endpoint is monitored (/health)
#
# STORAGE:
# [ ] R2_ACCOUNT_ID, R2_ACCESS_KEY_ID, R2_SECRET_ACCESS_KEY, R2_BUCKET_NAME configured
# [ ] R2_PUBLIC_URL set (optional but recommended for performance)
# [ ] R2 bucket has public access enabled (if using R2_PUBLIC_URL)
# [ ] Media directories exist and are writable (for local temp processing)
# [ ] Cleanup job is scheduled (cron or systemd timer)
# [ ] Sufficient disk space for temporary media processing
